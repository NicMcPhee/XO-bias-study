% Research Paper for GECCO 2015
% by Nic McPhee, Kirbie Dramdahl, and David Donatucci

\documentclass{sig-alternate}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\sloppy

\newcommand{\citep}[1]{\cite{#1}}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\begin{document}

\conferenceinfo{GECCO'15,} {July 11-15, 2015, Madrid, Spain.}
\CopyrightYear{2015}
\crdata{TBA}
\clubpenalty=10000
\widowpenalty = 10000
    
\title{Impact of Crossover Bias in Genetic Programming}

\numberofauthors{1}
\author{
\alignauthor
Nicholas Freitag McPhee, M. Kirbie Dramdahl, David Donatucci\\
	\affaddr{Division of Science and Mathematics}\\
	\affaddr{University of Minnesota, Morris}\\
	\affaddr{Morris, MN USA-56267}\\
	\email{\{mcphee, dramd002, donat056\}@morris.umn.edu}
}

% This is more like how it "should" be done, but I think the previous approach might look nicer. They
% may force us to change it, though, to make it easier to scrape information.

%\numberofauthors{3}
%\author{
%\alignauthor
%Nicholas Freitag McPhee\\
%	\affaddr{Division of Science and Mathematics}\\
%	\affaddr{University of Minnesota, Morris}\\
%	\affaddr{Morris, MN USA-56267}\\
%	\email{mcphee@morris.umn.edu}
%\alignauthor
%M. Kirbie Dramdahl\\
%	\affaddr{Division of Science and Mathematics}\\
%	\affaddr{University of Minnesota, Morris}\\
%	\affaddr{Morris, MN USA-56267}\\
%	\email{dramd002@morris.umn.edu}
%\alignauthor
%David Donatucci\\
%	\affaddr{Division of Science and Mathematics}\\
%	\affaddr{University of Minnesota, Morris}\\
%	\affaddr{Morris, MN USA-56267}\\
%	\email{donat056@morris.umn.edu}
%}

\date{} 
    
\maketitle

\begin{abstract}

\emph{\scriptsize This is probably too long. People often recommend keeping the abstract to somewhere between 150 and
250 words, and this is closer to 500. For the initial submission that's OK, but we may want to move some of this to the
introduction and trim down the abstract somewhat.}

In tree-based genetic programming with sub-tree crossover, the parent contributing the root portion of the tree (which
we refer to as the \emph{root parent}) often contributes more to the semantics of the resulting child than the other
parent (the \emph{non-root parent}). In previous research, we found that when the root parent had greater fitness than
the non-root parent, the fitness of the child tended to be better than if the reverse were true. Here we explore the
significance of that asymmetry by introducing the notion of \emph{crossover bias}, which allows us to bias the system
in favor of having the more fit parent be the root parent. To better understand the impact of this bias, we implemented
several levels of crossover bias, including 0\% bias (root individual chosen randomly, as in traditional sub-tree
crossover), 100\% bias (the stronger parent is always chosen to be the root parent), 50\% bias (bias implemented in
half the cases, and the other half chosen randomly), and reverse bias (the weaker parent is always chosen as root
parent). 

We applied crossover bias to a variety of problems. In most cases we found that using crossover bias either improved
performance or had no impact. Our results do, however, indicate the possibility that crossover bias may increase
selection pressure and premature convergence -- undesirable behavior, as it encourages a genetic programming run to
arrive at a solution too quickly, in the process potentially excluding more accurate solutions for a more generalized
one.

Our results also demonstrate that the effectiveness of crossover bias is somewhat dependent on the problem, and
significantly dependent on other parameter choices. In particular it appears that crossover bias has the largest impact
when selection pressure is weaker, and the differences in the fitness of the parents is thus likely to be larger. We
also found that the use of elitism reduced the influence of crossover bias. It's possible that crossover bias acts to
some degree as an ``elitism'' operator, making it more likely that the semantics of more fit individuals are copied
into the next generation; thus if traditional elitism is being employed this effect is less visible. Another possible
explanation for this is that if the most fit individuals are automatically being carried over, there is perhaps less
need to produce new, fitter individuals via crossover, reducing or even eliminating the usefulness of crossover bias.
Other factors which we found to have potential impact on the effectiveness of crossover bias were tournament size,
population size, and possibly the difference in parental fitness.

\end{abstract}

\category{}{}{}
\terms{}
\keywords{genetic programming, crossover bias, root parent}

\section{Introduction} \label{sec:Introduction}

As Figure~\ref{fig:root_parent_illustration} illustrates, the widely used sub-tree crossover operator in tree-based 
genetic programming (GP)~\cite{poli08:fieldguide} is an inherently
asymmetric operation, with one parent contributing the root node and, in most cases, a substantially larger
number of nodes than the other parent. This asymmetry has been noted before, e.g.,~\cite{mcphee1999analysis}
where they refer to the parent that contributes the root node as the \emph{root parent} and the other parent as
the \emph{non-root parent}.
It has also been previously observed 
\cite{McPheeDonatucciDramdahl:2014, McPhee:2008:SBB:1792694.1792707} 
that the root parent often contributes more to the semantics of the resulting 
individual. This is in part because of the tendency of the root parent to contribute more 
overall nodes, but also because the root node (and the nodes near it) frequently have an especially 
strong impact on the result of evaluating the tree in question. While the specific impact of this asymmetry will 
depend on the details of the function set and the particular trees chosen as parents, in many common 
tree-based GP settings the asymmetry does have a substantial effect on the semantics of the offspring.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Root_parent_illustration_no_triangle.pdf}
\caption{Sub-tree crossover, illustrating the asymmetric role of the \emph{root} and \emph{non-root} parents.}
\label{fig:root_parent_illustration}
\end{figure}

An important impact of this asymmetry was recently noted in~\cite{McPheeDonatucciDramdahl:2014},
where they observed that the fitness of the offspring tended to be better when the root parent had a better 
fitness than
the non-root parent.
To explore this further, we implemented what we refer to \emph{crossover bias}, which allowed us to
probabilistically force the genetic programming system to assign the individual with the greater 
fitness to be the root parent. Here we apply crossover bias (defined in Section~\ref{sec:XObias}) 
to a variety of test problems (described in Section~\ref{sec:Experiments}). The results 
(Section~\ref{sec:Results}) make it clear that crossover bias can have a substantial impact on the 
performance of GP runs, but that the results are heavily parameter dependent and, to a lesser degree 
problem dependent. In general, however, we find (Section~\ref{sec:Discussion}) that incorporating 
some crossover bias is, at least in these problems, usually either advantageous or neutral. 

\textbf{Should this next paragraph be moved the Conclusions section instead of having it here?}

While this paper focuses on the asymmetry in the context of tree-based GP and sub-tree crossover, it's important to 
note that asymmetries like this are common in many evolutionary systems, both biological and artificial. Much 
eukaryote reproduction is sexual, and brings with it numerous sex-linked traits and related asymmetries; this may
play an important role in speciation~\cite{qvarnstrom2009speciation}, arguably one of the most crucial of 
biological asymmetries.
Many evolutionary computation systems other than tree-based GP also have 
significant asymmetries. Many linear GP systems~\cite{brameier2007linear} and stack-based GP 
systems~\cite{spector:2002:GPEM}, for example, have asymmetries where the last instructions executed can have a 
disproportionate impact on the results. Similarly, changes near the front of grammatical evolution 
\cite{o2003grammatical} strings will have a 
disproportionate impact by determining the important early choices in the grammar productions. 
The results presented here suggests that it may be important to more thoroughly explore the impact of these asymmetries, both to better understand the performance and behavior of existing systems, but also as an aid to the design and discovery of new tools like recombination operators that leverage these asymmetries.

\section{Crossover Bias} \label{sec:XObias}

For this work, we implemented crossover bias with a parameter specifying the probability of forcing 
the more fit parent to be the root parent when performing crossover, as detailed in 
Algorithm~\ref{alg:biasAlgorithm}. Every time a 
crossover event was to be performed, we would use this probability to decide whether to force the 
more fit parent to be the root parent. Note that if the random value in Algorithm~\ref{alg:biasAlgorithm} 
didn't cause crossover bias to be applied, then the parents were left in their original random order, 
meaning that there is a 50\% chance that the root 
parent is the more fit parent even without any crossover bias.

\begin{algorithm}
\begin{algorithmic}
\Require RP and NRP are two individuals chosen as parents, with RP initially (and randomly) chosen to be the root parent.
\If {\Call{random}{0, 1} $\leq \textrm{XO\_bias}$} \Comment{Do we force bias?}
    \If {\Call{fitness}{RP} \textrm{worse than} \Call{fitness}{NRP}}
        \State RP, NRP := NRP, RP \Comment{Swap parents so RP is more fit} 
    \EndIf
\EndIf
\end{algorithmic}
\caption{Crossover bias}
\label{alg:biasAlgorithm}
\end{algorithm}

In this paper we focus on five levels of crossover bias:
\begin{itemize}
\item 0.00 bias - no crossover bias is applied;
\item 0.25 bias - system forced to use the more fit parent as the root parent in 25\% of cases;
\item 0.50 bias - system forced to use the more fit parent as the root parent in 50\% of cases;
\item 0.75 bias - system forced to use the more fit parent as the root parent in 75\% of cases; and
\item 1.00 bias - system forced to always use the more fit parent as the root parent, sometimes referred to as ``with crossover bias''.
\end{itemize}
The 0.00 case, where no crossover bias is applied, is just standard sub-tree crossover, where there will be a 50\% chance of the root parent being the more fit parent.

We also experimented with \emph{reverse} bias, where the weaker parent was always forced to be 
the root parent. In general, however, the results with reverse bias were effectively the same as using 
no bias (i.e., differences between reverse bias and no bias weren't statistically significant), so in the 
interest of simplification we've omitted reverse bias from the remainder of the paper.

\section{Experimental Setup} \label{sec:Experiments}

To better understand the impact of crossover bias on genetic programming performance, we experimented with five problems:
\begin{itemize}
	\item K Landscapes, with $K=6$ \cite{vanneschi2011k}
	\item Order Tree \cite{}
	\item U.S. Change
	\item Sine regression \cite{poli08:fieldguide}
	\item Pagie-1 regression \cite{}
\end{itemize}
Three of these (K Landscapes, Order Tree, and Pagie-1 regression) are taken from recent benchmark suggestions~\cite{gp-benchmarks-2013}. U.S. Change is a program synthesis problem taken from~\cite{USChangeSource}, and sine regression is the example problem used in earlier work on the impact of root and non-root parents~\cite{McPheeDonatucciDramdahl:2014}.

\textbf{Add a footnote about the two function sets for Pagie-1 and their impact.}

In the following sections, the terms \emph{``bias-effective settings''} and \emph{``non-bias-effective settings''} 
are used to
describe specific subsets of the parameters settings described above. As is seen in Section~\ref{sec:Results}, 
there are parameter choices where crossover bias appears to have a fairly strong impact, and other choices 
where crossover bias has almost no effect on the results. The ``bias-effective settings'' are:
\begin{itemize}
\item binary tournaments,
\item no elitism, and
\item population size of 10,240;
\end{itemize}
Non-bias-effective settings, conversely, are:
\begin{itemize}
\item tournament sizes of seven individuals,
\item elitism of either 0.1\% or 1\%, and
\item population size of 1,024;
\end{itemize}

All experiments were run using a copy of ECJ 21\footnote{\url{http://cs.gmu.edu/~eclab/projects/ecj/}} 
that we modified to support crossover bias.\footnote{Our intent is to submit our modifications back to the 
ECJ group to make it easy for other ECJ users to experiment with crossover bias.}

\section{Results} \label{sec:Results}

Unless noted otherwise, all mentions of statistical significance in this section are based on 
pairwise Wilcoxon tests with Holm corrections. All statistics calculations were performed with R~\cite{R} 
and plots were generated using the ggplot2 package~\cite{ggplot2Book}.

\textbf{Talk about the fact that we used pairwise Wilcoxon test with Holm corrections for the bulk of our statistical 
tests, along with a reference/citation to R as generated inside R. I'm not sure if we want to mention pairwise test of 
proportions (chi-squared) or not, because I'm not sure how much we'll use it in the paper.}

\subsection{Structural Problems}

\subsubsection{K Landscapes Problems}

We did a full sweep of parameters for the K Landscapes problem with both $K=2$ and $K=6$, 
using various combinations of the following:

\begin{itemize}
	\item crossover biases of -1.00, 0.00, 0.25, 0.50, 0.75, and 1.00;
	\item population sizes of 1,024 and 10,240;
	\item elitism percentages of 0\% or 1\%; and
	\item tournament sizes of 2, 3, 5, and 7.
\end{itemize}

Figure~\ref{fig:KLandscapes6_results} shows the impact of crossover bias on this problem across all the combinations of
parameter values (excepting reverse bias). Increasing the amount of crossover bias consistently improves the fitness of
the results. All the differences are statistically significant ($p < 0.012$) except for the difference between bias
probability 0.75 and 1.00.

%> pairwise.wilcox.test(klandscapes6$Fitness, klandscapes6$Bias.probability)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  klandscapes6$Fitness and klandscapes6$Bias.probability 
%
%     -1      0       0.25    0.5     0.75   
%0    0.99997 -       -       -       -      
%0.25 1.8e-06 1.8e-06 -       -       -      
%0.5  1.7e-12 2.3e-12 0.01136 -       -      
%0.75 < 2e-16 < 2e-16 6.0e-10 0.00032 -      
%1    < 2e-16 < 2e-16 3.7e-15 2.2e-07 0.15773
%
%P value adjustment method: holm 

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/KLandscapes6_XO_bias_impact_transformed_boxplot_alpha075.pdf}
\caption{Impact of crossover bias on fitness for K Landscapes problem, $K=6$ for a variety of treatments.}
\label{fig:KLandscapes6_results}
\end{figure}

Figure~\ref{fig:KLandscapes6_strong_results} filters out only the data gathered using the bias-effective treatment,
discussed in Section~\ref{sec:Experiments}. It is clear that the impact of crossover bias is much stronger in this case
than in the more general case shown in Figure~\ref{fig:KLandscapes6_results}. Here all the differences are strongly
statistically significant ($p < 10^{-11}$), with the exception of the difference between reverse bias (-1.00; not shown)
and no bias (0.00). In addition to improvements in fitness, increasing the crossover bias also increases the number of
``perfect'' solutions discovered. Out of 100 runs with a crossover bias setting of 1.00, 15 of these runs resulted in
the discovery of a ``perfect'' solution. By comparison, only 1 or 2 runs out of 100 resulted in ``perfect'' solutions
for each of the other crossover probabilities. This difference is statistically significant with $p \leq 0.03$ using a
pairwise test of proportions.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/KLandscapes6_XO_bias_strong_impact_alpha_075.pdf}
\caption{Impact of crossover bias on fitness for K Landscapes problem, $K=6$, restricted to binary tournament
selection, no elitism, and population size 10,240.}
\label{fig:KLandscapes6_strong_results}
\end{figure}

%> pairwise.wilcox.test(strong$Fitness, strong$Bias.probability)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  strong$Fitness and strong$Bias.probability 
%
%     -1      0       0.25    0.5     0.75   
%0    0.18    -       -       -       -      
%0.25 < 2e-16 < 2e-16 -       -       -      
%0.5  < 2e-16 < 2e-16 < 2e-16 -       -      
%0.75 < 2e-16 < 2e-16 < 2e-16 < 2e-16 -      
%1    < 2e-16 < 2e-16 < 2e-16 < 2e-16 2.3e-12
%
%P value adjustment method: holm 

%> countSuccesses(strong)
%[1]  1  2  2  1  2 15
%> pairwise.prop.test(countSuccesses(strong), rep(100, 6))
%
%	Pairwise comparisons using Pairwise comparison of proportions 
%
%data:  countSuccesses(strong) out of rep(100, 6) 
%
%  1     2     3     4     5    
%2 1.000 -     -     -     -    
%3 1.000 1.000 -     -     -    
%4 1.000 1.000 1.000 -     -    
%5 1.000 1.000 1.000 1.000 -    
%6 0.011 0.030 0.030 0.011 0.030
%
%P value adjustment method: holm 

\subsubsection{OrderTree Problem}

For the OrderTree problem, a full sweep of the following parameters was used:

\begin{itemize}
	\item crossover biases of -1.00, 0.00, 0.25, 0.50, 0.75, and 1.00;
	\item population size of 1,024;
	\item elitism percentages of 0\% or 1\%; and
	\item tournament sizes of 2, 3, 5, and 7.
\end{itemize}

While similar to the parameters used for the K Landscapes problem, one difference is apparent: the population size for
the OrderTree runs was limited to 1,024. This was a consequence of the large trees generated for this problem,
resulting in out of memory errors for the larger population size of 10,240.

Initial runs of this problem with (1.00) and without (0.00) crossover bias across all four tournament sizes
demonstrated that in each case, adding crossover bias improved fitness. Additionally, these improvements were
statistically significant ($p < 0.002$ for each pairing using a pairwise Wilcoxon with Holm correction).

However, in the next set of runs, using the full range of crossover bias values but limited to binary tournaments, we
observed a drop in fitness from bias 0.75 to bias 1.00, actually dropping slightly below that for 0.50 as well. All
these differences are statistically significant ($p < 0.03$), with the exception of the difference between bias 0.25
and bias 1.00, so bias 0.75 is the clear winner in this scenario. \textbf{I (Nic) suspect this is actually quite
interesting and might open interesting doors to things like premature convergence and need for being able to have a
least a little variation in the system for a problem like this. Maybe we should talk about that in the "Future work"
section?}

Figure~\ref{fig:Ordertree_results_all_tournaments_Jan15} generalizes this to cover all four tournament sizes. The
figure demonstrates that the drop in fitness for bias 1.00 observed for binary tournaments is consistent across the
other three tournament sizes as well. However, it is also apparent from the figure that in general the impact of
crossover bias lessens with the larger tournament sizes, both in the increase in fitness up to bias 0.75, and the drop
from there to bias 1.00.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Ordertree_results_all_tournaments_Jan15.pdf}
\caption{Impact of crossover bias on fitness for OrderTree problem for multiple tournament sizes.}
\label{fig:Ordertree_results_all_tournaments_Jan15}
\end{figure}

Compare this to the results shown in Figure~\ref{fig:KLandscapes6_XO_bias_impact_facets}, which plots the corresponding
data from the K Landscapes runs. It is clear that for binary tournaments, increasing the crossover bias probability
continues to improve the fitness - all the differences are strongly statistically significant ($p<6e-08$). This
continues to be true for tournament size 3 - all the differences are statistically significant except for that between
bias 0.50 and 0.75 and that was very close ($p=0.05620$). None of the differences for tournament size 5 are
significant, and so we will past over it here. For tournament 7, however, it does look like the reverse is true, where
increasing crossover actually hurts fitness. Almost none of the differences for tournament size 7 are statistically
significant, however, with the only exception being the difference between 0.25 and 1.00 ($p=0.21$ using a pairwise
Wilcoxon test with Holm correction). \textbf{This paragraph might move to the Discussion or Conclusions section if we
keep it, or something like it.}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/KLandscapes6_XO_bias_impact_facets.pdf}
\caption{Impact of crossover bias on fitness for K-Landcapes problem, K=6, for various tournament sizes. \textbf{Move this into the K Landscapes section next to Figure 2, along with the associated discussion.}}
\label{fig:KLandscapes6_XO_bias_impact_facets}
\end{figure}

%> pairwise.wilcox.test(subset(klandscapes6, Tourny.size==7)$Fitness, subset(klandscapes6, Tourny.size==7)$Bias.probability)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  subset(klandscapes6, Tourny.size == 7)$Fitness and subset(klandscapes6, Tourny.size == 7)$Bias.probability 
%
%     0     0.25  0.5   0.75 
%0.25 1.000 -     -     -    
%0.5  1.000 1.000 -     -    
%0.75 1.000 0.556 1.000 -    
%1    0.075 0.021 0.556 1.000
%
%P value adjustment method: holm 

\subsection{U.S. Change Problem}

In contrast to the structural problems discussed in the previous section, which used fitness as the measure of success
of a run, for the U.S. Change problem measured success in ``hits'' (the number of test cases that are correctly
solved). There are 150 test cases in our implementation, so an optimal program will have a hits score of 150.

Figure~\ref{fig:USChange_Hits} shows the impact of crossover bias on the number of hits for the U.S. Change problem
across the full collection of parameter settings. This suggests that in general there is little consistent impact of
crossover bias, but a pairwise Wilcoxon test with Holm correction indicates that while most of the differences in this
plot are not statistically significant, two are: the differences between crossover bias 0.00 and crossover bias 0.50 and
0.75 are both statistically significant ($p \leq 0.015$), even if numerically small.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/US_change_hits.pdf}
\caption{Impact of crossover bias on the number of hits for the U.S. Change problem across a variety of treatments.}
\label{fig:USChange_Hits}
\end{figure}

%> pairwise.wilcox.test(us_change$Hits, us_change$Bias, conf.int=TRUE)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  us_change$Hits and us_change$Bias 
%
%     0     0.25  0.5   0.75 
%0.25 0.117 -     -     -    
%0.5  0.011 1.000 -     -    
%0.75 0.015 1.000 1.000 -    
%1    0.083 1.000 1.000 1.000
%
%P value adjustment method: holm 

However; if we limit our attention to binary tournaments, no elitism, and the larger population size of 10,240; then we
find that crossover bias has a substantial and statistically significant impact, as is seen in
Figure~\ref{fig:USChange_Hits_strong}. Here the bulk of these pairwise differences are statistically significant
($p<0.0002$ using a pairwise Wilcoxon test with Holm correction). The major exception is the difference between
crossover bias 0.75 and 1.00 ($p=0.43078$). Two other adjacent pairs have $p$-values slightly above 0.05: crossover bias
0.25 \emph{vs.} 0.50 ($p=0.05036$), and 0.50 \emph{vs.} 0.75 ($p=0.08019$).

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/US_change_hits_strong.pdf}
\caption{Impact of crossover bias on the number of hits for the U.S. Change problem, limited to binary 
tournaments, no elitism, and population size 10,240.}
\label{fig:USChange_Hits_strong}
\end{figure}

%> pairwise.wilcox.test(us_change_strong$Hits, us_change_strong$Bias)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  us_change_strong$Hits and us_change_strong$Bias 
%
%     0       0.25    0.5     0.75   
%0.25 0.00013 -       -       -      
%0.5  3.4e-09 0.05036 -       -      
%0.75 1.1e-12 0.00016 0.08019 -      
%1    3.2e-15 5.1e-06 0.01652 0.43078
%
%P value adjustment method: holm 

If complete success was considered vital (which might be the case if we were evolving software for use in a production
system), then it would make sense to see if crossover bias has a significant impact on the success rate.
Figure~\ref{fig:USChange_Successes_strong} shows the number of successes for the various crossover bias values when
using binary tournaments, no elitism, and population size 10,240. A pairwise test of proportions (chi-squared) with
Holm correction indicates that while non of the adjacent differences (for example, crossover bias 0.50 \emph{vs.} 0.75)
are statistically significant, most of the non-adjacent differences (for example, crossover bias 0.25 \emph{vs.} 0.75)
are statistically significant, with the exceptions being crossover bias 0.00 \emph{vs.} 0.50, and bias 0.50 and 1.00
($p=0.09361$ in both cases).

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/US_change_successes_strong.pdf}
\caption{Impact of crossover bias on the number of successes runs for the U.S. Change problem when using binary
tournaments, no elitism, and population size 10,240.}
\label{fig:USChange_Successes_strong}
\end{figure}

%> pairwise.prop.test(c(6, 12, 18, 30, 34), rep(100, 5))
%
%	Pairwise comparisons using Pairwise comparison of proportions 
%
%data:  c(6, 12, 18, 30, 34) out of rep(100, 5) 
%
%  1       2       3       4      
%2 0.65003 -       -       -      
%3 0.09361 0.65003 -       -      
%4 0.00021 0.02215 0.27429 -      
%5 1.8e-05 0.00334 0.09361 0.65003
%
%P value adjustment method: holm 

%\begin{figure}
%\centering
%\includegraphics[width=0.45 \textwidth]{Plots/US_change_Bias_impact_vs_success.png}
%\caption{Relationship between proportion of successful runs and the impact of crossover bias.}
%\label{fig:USChangeBiasImpactVsSuccess}
%\end{figure}

\subsection{Symbolic Regression Problems}

\textbf{We decided that we're going to just focus on hits for both regression problems, which means we should remove the fitness plots/discussion from the Pagie-1 section.}

\subsubsection{Pagie-1 Problem}

\textbf{We agreed to just drop everything about the koza2 function set, which includes redrawing Figure~\ref{fig:Pagie1Hits_Bias_Tournys_FunctionSet} to only have basic4.}

For the Pagie-1 problem we did runs across the following parameters:

\begin{itemize}
	\item crossover biases of -1.00, 0.00, 0.25, 0.50, 0.75, and 1.00;
	\item population sizes of 1,024 and 10,240;
	\item elitism percentages of 0\% or 1\%;
	\item tournament sizes of 2, 3, 5, and 7;
	\item basic4 ($+, -, \times, \div$) and koza2 (complete) function sets; and
	\item with and without Tarpeian bloat control.
\end{itemize}

However, for purposes of space conservation, we will not discuss the koza2 function set results here.

Figure~\ref{fig:Pagie1Hits_Bias_Tournys_FunctionSet} shows the impact of crossover bias on the number of hits for the
Pagie-1 regression problem, separated out by both tournament size and function set used;
Figure~\ref{fig:Pagie1Fitness_Bias_Tournys_FunctionSet} is similar, but plots fitness (total error) instead of hits.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Pagie_1_Hits_vs_Bias_Tournys_FunctionSet.pdf}
\caption{Impact of crossover bias on the number of hits for the Pagie-1 symbolic regression problem, broken out for the
four different tournament sizes (2, 3, 5, and 7) and the two different function sets (basic4 and koza2). The maximum
number of possible hits is 676.}
\label{fig:Pagie1Hits_Bias_Tournys_FunctionSet}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Pagie_1_Fitness_vs_Bias_Tournys_FunctionSet.pdf}
\caption{Impact of crossover bias on the fitness (total error) for the Pagie-1 symbolic regression problem, broken out 
for the four different tournament sizes (2, 3, 5, and 7) and the two different function sets (basic4 and koza2). The
best possible is an error of 0.}
\label{fig:Pagie1Fitness_Bias_Tournys_FunctionSet}
\end{figure}

The results, as can be seen in Figures~\ref{fig:Pagie1Hits_Bias_Tournys_FunctionSet}
and~\ref{fig:Pagie1Fitness_Bias_Tournys_FunctionSet} vary significantly. However, if limited to the following
parameters:
\begin{itemize}
    \item crossover biases of -1.00, 0.00, 0.25, 0.50, 0.75, and 1.00;
    \item population size 10,240;
    \item no elitism;
    \item tournament sizes of 2, 3, 5, and 7;
	\item basic4 ($+, -, \times, \div$) function set; and
	\item Tarpeian bloat control;
\end{itemize}
then the complexities of Figures~\ref{fig:Pagie1Hits_Bias_Tournys_FunctionSet}
and~\ref{fig:Pagie1Fitness_Bias_Tournys_FunctionSet} simplify to
Figures~\ref{fig:Pagie1StrongHits_Bias_Tournys_FunctionSet} and~\ref{fig:Pagie1StrongFitness_Bias_Tournys_FunctionSet}.
For now we will focus on this specific subset of the data.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Pagie_1_strong_Hits_vs_Bias_Tournys_FunctionSet.pdf}
\caption{Impact of crossover bias on the number of hits for the Pagie-1 symbolic regression problem, broken out by 
tournament size (2, 3, 5, and 7). These runs use the ``basic4'' function set ($+, -, \times, \div$), no elitism, 
population size 10,240, and Tarpeian bloat control. The maximum number of possible hits is 676.}
\label{fig:Pagie1StrongHits_Bias_Tournys_FunctionSet}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Pagie_1_strong_Fitness_vs_Bias_Tournys_FunctionSet.pdf}
\caption{Impact of crossover bias on the number of fitness (total error) for the Pagie-1 symbolic regression problem, 
broken out by tournament size (2, 3, 5, and 7). These runs use the ``basic4'' function set ($+, -, \times, \div$), no 
elitism, population size 10,240, and Tarpeian bloat control. The best possible is an error of 0.}
\label{fig:Pagie1StrongFitness_Bias_Tournys_FunctionSet}
\end{figure}

Focusing on the data in Figure~\ref{fig:Pagie1StrongHits_Bias_Tournys_FunctionSet}, the differences between crossover
bias 0.00 (standard subtree crossover) and all the other positive crossover bias values are statistically significant
($p<10^{-12}$), and all of the differences between crossover bias 0.25 and higher biases are significant ($p<0.007$);
however, none of the differences among 0.50, 0.75, and 1.00 are statistically significant. None of the differences for
tournament sizes 3, 5, or 7 are statistically significant, although the difference between crossover biases 0.00 and
1.00 are very close ($p=0.053$) for tournament size 7. This indicates that for binary tournaments, including crossover
bias substantially and significantly improves the hit rate, although all bias rates above 0.25 are very similar. For
tournament size 7, however, it appears that adding crossover bias tends to reduce the hit rate, although in a less
substantial and significant manner.

%> pairwise.wilcox.test(subset(pagie1_strong, Tourny.size==2)$Hits, subset(pagie1_strong, Tourny.size==2)$Bias)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  subset(pagie1_strong, Tourny.size == 2)$Hits and subset(pagie1_strong, Tourny.size == 2)$Bias 
%
%     0       0.25    0.5     0.75   
%0.25 1.7e-13 -       -       -      
%0.5  < 2e-16 0.00639 -       -      
%0.75 < 2e-16 0.00076 1.00000 -      
%1    < 2e-16 0.00037 1.00000 1.00000
%
%P value adjustment method: holm 

The statistical significance in Figure~\ref{fig:Pagie1StrongFitness_Bias_Tournys_FunctionSet} is very similar to that
in the previous figure (Figure~\ref{fig:Pagie1StrongHits_Bias_Tournys_FunctionSet}). Here again adding bias for binary
tournaments improves fitness, but none of the differences between biases 0.50, 0.75, and 1.00 are statistically
significant. None of the differences for tournament sizes 3 and 5 are significant, and most of those for tournament
size 7 are not significant. The one exception for tournament size 7 is that the difference between crossover biases
0.00 and 1.00 is statistically significant ($p = 0.021$).

Figure~\ref{fig:Pagie1StrongSuccesses} shows the number of successes (runs that exactly solve the problem). Almost none
of these differences are statistically significant, with the major exception being the small number of successes (3)
for binary tournaments without bias, which is significantly different from all the other bias values for binary
tournaments ($p<0.0002$). The one other exception is for tournament size 7, the difference between the number of
successes with crossover biases 0.00 and 1.00 is significant ($p=0.015$).

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Pagie_1_Strong_Successes_vs_Bias.pdf}
\caption{Impact of crossover bias on the number of successes (runs that exactly solve the problem) for the Pagie-1
symbolic regression problem broken out by tournament size (2, 3, 5, and 7). These runs use the ``basic4'' function set
(just $+, -, \times, \div$), no elitism, population size 10,240, and Tarpeian bloat control.}
\label{fig:Pagie1StrongSuccesses}
\end{figure}

\textbf{Waiting for treatment information on Figures~\ref{fig:Pagie1FitnessOverTime} and~\ref{fig:Pagie1SizeOverTime}.
Figure~\ref{fig:Pagie1SizeOverTime} will eventually be removed, but the information is interesting, and I (Kirbie)
would like as much information on it as possible before I remove it.}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Pagie-1_fitness_vs_time.png}
\caption{Impact of crossover bias on the fitness over time for the Pagie-1 symbolic regression problem. \textbf{What's the data set this comes from?} \textbf{Drop this for now.}}
\label{fig:Pagie1FitnessOverTime}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Pagie-1_size_vs_time.png}
\caption{Impact of crossover bias on the tree size over time for the Pagie-1 symbolic regression problem. \textbf{Drop this for now.}}
\label{fig:Pagie1SizeOverTime}
\end{figure}

\subsubsection{Sine Problem}

For the sine regression problem, we limited our runs to two parameter treatments, referred to as bias-effective setting
and and non-bias-effective settings respectively, discussed in detail in Section~\ref{sec:Experiments}.

Figure~\ref{fig:sineBiasResultsStrong} shows the hits results for the sine regression problem using binary tournaments,
no elitism, and population size 10,240. All these differences are statistically significant ($p < 10^{-5}$ using a
pairwise Wilcoxon rank sum test) except for the difference between the bias of 0.75 and 1.00.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Sine_XO_impact_strong_boxplot.pdf}
\caption{Impact of crossover bias on the sine symbolic regression problem with bias-effective settings: binary
tournaments, no elitism, and population size 10,240.}
\label{fig:sineBiasResultsStrong}
\end{figure}

%> pairwise.wilcox.test(sine_strong_final$Hits, sine_strong_final$Bias)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  sine_strong_final$Hits and sine_strong_final$Bias 
%
%     0       0.25    0.5     0.75  
%0.25 1.9e-07 -       -       -     
%0.5  4.0e-15 0.0017  -       -     
%0.75 < 2e-16 1.2e-12 8.9e-06 -     
%1    < 2e-16 1.3e-13 2.9e-07 0.1938
%
%P value adjustment method: holm 

%> pairwise.wilcox.test(sine_strong_final$Standardized.fitness, sine_strong_final$Bias)
%
%	Pairwise comparisons using Wilcoxon rank sum test 
%
%data:  sine_strong_final$Standardized.fitness and sine_strong_final$Bias 
%
%     0       0.25    0.5     0.75 
%0.25 3.6e-10 -       -       -    
%0.5  < 2e-16 2.7e-05 -       -    
%0.75 < 2e-16 5.0e-14 1.1e-05 -    
%1    < 2e-16 < 2e-16 2.9e-09 0.072
%
%P value adjustment method: holm 

Figure~\ref{fig:sineBiasResultsWeak} shows the results for the sine regression problem using tournament size 7, 0.1\%
elitism, and smaller populations  of size 1,024. None of these differences are statistically significant using a
pairwise Wilcoxon rank sum test.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Sine_XO_impact_weak_boxplot.pdf}
\caption{Impact of crossover bias on the sine symbolic regression problem with non-bias-effective settings: tournament
size 7, 0.1\% elitism, and population size 1,024.}
\label{fig:sineBiasResultsWeak}
\end{figure}

Figures~\ref{fig:sineBiasFitnessVsGenStrong} and~\ref{fig:sineBiasFitnessVsGenWeak} show the change in fitness over
time for the bias-effective and non-bias-effective configurations. In the bias-effective configuration we get a really
clean ``adding bias is good'' demonstration. In the non-bias-effective configuration, the impact of crossover bias is
quite minimal; \textbf{oddly, though, it does look like a crossover bias of 1.00 is worse by a little bit than
everything else at the end, which is weird}. Figure~\ref{fig:sineBiasFitnessVsGenT2E01P10K} shows fitness over time for
the non-bias-effective configuration with population size 10,240 (instead of 1,024 for the ``normal'' non-bias-effective
configuration). Increasing the population size improves the performance significantly. The different bias levels are
all essentially the same at the end of the 100 generations, but there is definitely a spread around 15-20 generations
that is a really clean ``more bias is better'' demonstration (\textbf{Do we care?}) except for the fact that 0.75 and
1.00 are essentially the same. \textbf{Do we want to include both Figures~\ref{fig:sineBiasFitnessVsGenWeak}
and~\ref{fig:sineBiasFitnessVsGenT2E01P10K}? Do we want to combine them into a single graph (either just one graph, or
as two ``panels'' like, e.g., in Figure~\ref{fig:parentErrorsSine}?}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Sine_XO_fitness_vs_gen_strong.pdf}
\caption{Impact of crossover bias on the sine symbolic regression problem with binary tournaments, no elitism, and 
population size 10,240. \textbf{Move this to where we define bias-effective parameters.}}
\label{fig:sineBiasFitnessVsGenStrong}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Sine_XO_fitness_vs_gen_weak.pdf}
\caption{Impact of crossover bias on the sine symbolic regression problem with tournament size 7, 0.1\% elitism, and 
population size 1,024.  \textbf{Move this to where we define non-bias-effective parameters, combined with the previous figure as a pair of panels.}}
\label{fig:sineBiasFitnessVsGenWeak}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Sine_XO_fitness_vs_gen_t2_e01_p10K.pdf}
\caption{Impact of crossover bias on the sine symbolic regression problem with tournament size 7, 0.1\% elitism, and 
population size 10,240. \textbf{Drop this.}}
\label{fig:sineBiasFitnessVsGenT2E01P10K}
\end{figure}

\section{Discussion} \label{sec:Discussion}

Why does all this happen this way? For example, why does crossover bias have a much stronger effect when using binary
tournaments than when using larger tournament sizes such as 7. One possible explanation is that with large tournaments,
the difference in fitness between the two parents is likely to be closer, because the larger tournaments help ensure
that both parents are from the more highly fit part of the population. To better understand this, blah, blah, blah
\textbf{We need to turn this into actual text}.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Parent_errors_sine.pdf}
\caption{Plot of the errors of all individuals chosen as parents from some sine runs. \textbf{Explain this.} \textbf{Do
we want/need this plot?} \textbf{We agreed to get rid of this.}}
\label{fig:parentErrorsSine}
\end{figure}

Figure~\ref{fig:parentDiffsSine} shows the distribution of relative difference in parent errors in the sine regression 
problem. For each crossover event, the relative difference in parent errors is
\[
	|e_A - e_B] / (e_A + e_B)
\]
where $e_A$ and $e_B$ are the errors of the two chosen parents $A$ and $B$. This has a minimum value of 0 when 
the two errors are the same, and a maximum value approaching 1 for the case where one of the errors is nearly 0.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Parent_normalized_error_diffs_sine.pdf}
\caption{Plot of the normalized differences in parent errors from some sine runs. \textbf{Explain this}.}
\label{fig:parentDiffsSine}
\end{figure}

We see similar results for the K Landscapes problem, as illustrated in Figures~\ref{fig:parentFitnessesKLandscapes}
and~\ref{fig:parentDiffsKLandscapes}.

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Parent_fitnesses_KLandscapes.pdf}
\caption{Plot of the fitnesses of all individuals chosen as parents from some K Landscape runs. \textbf{Explain this.}
\textbf{Do we want/need this plot?} \textbf{We agreed to get rid of this.}}
\label{fig:parentFitnessesKLandscapes}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45 \textwidth]{Plots/Parent_normalized_fitness_diffs_KLandscapes.pdf}
\caption{Plot of the normalized differences in parent fitnesses from some K Landscape runs. \textbf{Explain this}.}
\label{fig:parentDiffsKLandscapes}
\end{figure}

\section{Conclusions} \label{sec:Conclusions}

In most of these experiments we found better results with tournament sizes of 7 than with binary tournaments, and in 
general using larger tournaments appears to wash out much of the impact of crossover bias, so there's a fair question 
about whether one should just use larger tournaments and ignore crossover bias. \textbf{How do we respond to this? I 
think the answer is something like ``It doesn't hurt (at least in our experiments), and it sometimes helps, even for 
tournament size 7 and with elitism. For interesting problems, you also don't know in advance what your best parameter 
choices are, so it's at least worth including in your arsenal.''}

\textbf{Return to the paragraph in the intro about how asymmetries are very common and that these results suggest that we should look at their impact in other EC system.}
In general these 
EC system
asymmetries weren't intentional design goals, but were instead simple artifacts of other system design decisions, and 
the potential impact of these asymmetries has been largely unstudied.


% \section*{Acknowledgements}

% Many thanks to the members of the Hampshire College Computational Intelligence Lab for suggestions and feedback as this work developed, with particular thanks to Thomas Helmuth, William LaVaca, and Lee Spector. Thanks also to Thomas Helmuth for introducing us to the U.S. Change problem. Thanks to W. B. Langdon for valuable feedback and numerous suggestions.

\bibliographystyle{acm}
\bibliography{Research_2015}

\end{document}